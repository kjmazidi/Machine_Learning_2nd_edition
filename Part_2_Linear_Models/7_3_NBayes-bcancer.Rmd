---
title: "Naive Bayes with the Breast Cancer data"
author: "Karen Mazidi"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

In this notebook we compare Naive Bayes and logistic regression on the breast cancer data in package mlbench.

### Load the data

The breast cancer data is in the mlbench package. There are 669 observations with 11 columns. Column 1 is an ID that will be ignored later, columns 2-10 are factors specifying information gleaned from bioposies. The final column is the label: benign or malignant. The class distribution is 458 benign to 241 malignant, about 64% benign to 36% malignant. 

```{r}
library(mlbench)
data(BreastCancer)
str(BreastCancer)
summary(BreastCancer$Class)
```


### Divide data into train, test

First remove the Id column, then divide into 80% train, 20% test. 

```{r}
set.seed(1234)
df <- BreastCancer[,-1] # remove ID
i <- sample(1:nrow(df), 0.8*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]
```

### logistic regression

Build a logistic regression model.

```{r}
glm1 <- glm(Class~., data=train, family=binomial)
summary(glm1)
```

### Test

Evaluate on the test data. The logistic regression model gets 91% accuracy.

```{r}
probs1 <- predict(glm1, newdata=test, type="response")
pred1 <- ifelse(probs1>0.5, 2, 1)
print(table(pred1, test$Class))
acc1 <- mean(pred1==as.integer(test$Class), na.rm=TRUE)
acc1
```
 Examine the results using the caret package.
 
```{r}
library(caret)
confusionMatrix(factor(pred1), factor(as.integer(test$Class)))
```
 


### Build a Naive Bayes classifier

Use the same test and train data for comparison.

```{r}
library(e1071)
#nb1 <- naiveBayes(train[,-10], train[,10])
nb1 <- naiveBayes(Class~., data=train)
summary(nb1)
nb1
```

### Evaluate on the test data

The Naive Bayes model gets 96\% accuracy. 

```{r}
pred2 <- predict(nb1, newdata=test[,-10], type="class")
table(pred2, test$Class)
acc2 <- mean(pred2==test$Class)
acc2
```

Evaluate the results with the caret package.

```{r}
confusionMatrix(pred2, test$Class, positive="malignant")
```
### Set cut-off points for predictors

```{r}
df2 <- df[, c(2:3, 10)]
df2$Cell.size <- as.factor(ifelse(df$Cell.size > 5, 1, 0))
df2$Cell.shape <- as.factor(ifelse(df$Cell.shape > 5, 1, 0))
str(df2)

train2 <- df2[i,]
test2 <- df2[-i,]
```

### Re-run logistic regression

```{r}
glm2 <- glm(Class~., data=train2, family=binomial)
summary(glm2)
```

```{r}
probs3 <- predict(glm2, newdata=test2, type="response")
pred3 <- ifelse(probs3>0.5, 2, 1)
print(table(pred3, test$Class))
acc3 <- mean(pred3==as.integer(test2$Class), na.rm=TRUE)
acc3
```



### Re-run naive Bayes

```{r}
nb2 <- naiveBayes(Class~., data=train2)
nb2
```

```{r}
pred4 <- predict(nb2, newdata=test2, type="class")
table(pred4, test$Class)
acc4 <- mean(pred4==test$Class)
acc4
```

