---
title: "Multiple Linear Regression"
author: "Karen Mazidi"
output:
  pdf_document: default
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
---

ChickWeight is a built-in R data set with 578 rows and 4 columns of data resulting from an experiment on the effect of different types of feed on chick weight. Each observation (row) in the data set represents the weight in grams of a given chick on a given day, recorded in column Time. 

### Data exploration

Let's explore the data with R functions and plots.

```{r}
data(ChickWeight)
str(ChickWeight)
head(ChickWeight)
```



```{r}
par(mfrow=c(1,2))
plot(ChickWeight$Time, ChickWeight$weight,
     xlab="Time", ylab="Weight")
plot(ChickWeight$Diet, ChickWeight$weight,
     xlab="Diet", ylab="Weight")
```

### Divide the data into train and test sets

We randomly sample the rows to get a vector i with row indices. This is used to divide into train and test sets. 

```{r}
set.seed(1234)
i <- sample(1:nrow(ChickWeight), nrow(ChickWeight)*0.75, replace=FALSE)
train <- ChickWeight[i,]
test <- ChickWeight[-i,]
```

### Simple linear regression

In simple linear regression we have a single predictor variable for our target variable. Here we wish to see the impact of Time on weight.

```{r}
lm1 <- lm(weight~Time, data=train)
summary(lm1)
```

### Plotting the residuals

The 4 residual plots are placed in a 2x2 grid.

```{r}
par(mfrow=c(2,2))
plot(lm1)
```

### Multiple Linear Regression

If we have more than one predictor in linear regression we call it multiple linear regression. Here we want to see the effect of both Time and Diet on chick weight.

```{r}
lm2 <- lm(weight~Time+Diet, data=train)
summary(lm2)
```

The adjusted R-squared for lm2 is 0.7338, which is an improvement of lm1's 0.6863.

### The anova() function

The analysis of variance function here is used to compare the two models. We see that lm2 lowered the errors, RSS, and had a low p-value. These are indications that lm2 is a better model than lm1. 

```{r}
anova(lm1, lm2)
```


### Linear models are not always straight lines

Next we try predicting the log of weight to illustrate that linear models are not always straight lines. This damped down some of the variation in the residuals. The lm3 model had a higher R-squared of 0.8474. We cannot do anova() comparing lm3 because it has a different target, the log(weight) instead of weight.

```{r}
lm3 <- lm(log(weight)~Time+Diet, data=ChickWeight)
summary(lm3)
par(mfrow=c(2,2))
plot(lm3)
```

### Predict on test data with model 1

rmse = 30.44
cor = .87

```{r}
pred <- predict(lm1, newdata=test)
cor(pred, test$weight)
mse <- mean((pred-test$weight)^2)
rmse <- sqrt(mse)
```

### Predict on test data with model 2

cor is slightly better at .887
rmse is slightly better at 29.00

```{r}
pred <- predict(lm2, newdata=test)
cor(pred, test$weight)
mse2 <- mean((pred-test$weight)^2)
rmse2 <- sqrt(mse2)
```


### Predict on test data with model 3

correlation is better at .9289
rmse is significanlty better, at 0.2

```{r}
pred <- predict(lm3, newdata=test)
cor(pred, log(test$weight))
mse3 <- mean((pred-log(test$weight))^2) 
rmse3 <- sqrt(mse3)
```

Note that we can't do an anova comparison with model 3 because it has a target of log(weight) and lm1 and lm1 have weight as a target.